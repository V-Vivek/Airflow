# Core Components

## Scheduler
- As a workflow management platform, the core component that sits at the heart of airflow is a Scheduler. 
- It is responsible for triggering the DAGs as well as the tasks according to their scheduled time and dependencies. 
- It does that by monitoring tasks and kicking-off the downstream dependent tasks once the upstream tasks finish. 
- To do so, Scheduler submits the tasks to an executor.

## Executor
- An executor is a part of scheduler that handles and manages the running tasks. 
- Airflow provides different types of executors, namely the major ones are 2 single-node executors (local executor & Sequential executor) and 2 multi-node executors (Celery Executor & Kubernetes Executor).

## Worker
- A place where the tasks run. 
- This could be on the same machine/node where scheduler is running, if using single-node executors or a dedicated machine/node if using multi-node executors.

## Webserver
- A user interface where users can view, control and monitor all DAGs. 
- This interface provides functionality to trigger dag or a task manually, clear DAG runs, view task states & logs and view tasks run-duration. 
- It also provides the ability to manage users, roles and several other configurations for the Airflow setup.

## Metadata database
- A database that stores workflow states, run duration, logs locations etc. 
- This database also stores information regarding users, roles, connections, variables etc.

## Dags directory
- A location where airflow stores all DAG codes. 
- This is accessible to scheduler, webserver and workers.

## Logs directory
- A location where airflow store logs of all finished tasks. 
- Location address of each task-run is stored in metadata database. 
- User can then view the logs from this location via webserver UI. 
- Airflow can also be configured to set remote log directory e.g. S3 or GCS.

# Working  
![image](https://user-images.githubusercontent.com/117569148/232107081-22b17620-df93-430d-8b03-49bd70f50bbb.png)

1. A user first logins to the webserver interface to view and control workflows.
2. Webserver then retrieves all dags from dags directory as well as pull information from metadata database about dag states.
3. When a DAG gets triggered (whether automatically on its scheduled time or manually by user via webserver), a scheduler then submits the DAG tasks to executor. An Executor then submits these tasks to workers.
4. A single worker takes care of one task at a time. For this, a worker fetches DAG code from dags directory and runs.
5. Upon tasks completion, executor retrieves tasks state from worker and updates into metadata database.
6. Finally, executors fetches tasks logs from workers and persists them into logs directory, to be viewable from UI dashboard.
