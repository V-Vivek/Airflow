Let's consider a simple example DAG that represents a data processing workflow. This DAG has three tasks: task_a, task_b, and task_c, where task_b and task_c depend on the success of task_a.

```bash
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 9, 26),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
)

# Define tasks
task_a = DummyOperator(task_id='task_a', dag=dag)
task_b = PythonOperator(task_id='task_b', python_callable=lambda: print("Task B executed"), dag=dag)
task_c = PythonOperator(task_id='task_c', python_callable=lambda: print("Task C executed"), dag=dag)

# Set up task dependencies
task_a >> task_b
task_a >> task_c
```

Now, let's explain how each DAG trigger rule would affect the execution of this DAG:

## all_success
The DAG will run if both task_b and task_c are in a success state.
```bash
dag.trigger_rule = 'all_success'
```

## all_failed
The DAG will run if both task_b and task_c are in a failed state.
```bash
dag.trigger_rule = 'all_failed'
```

## one_success
The DAG will run if at least one of task_b or task_c is in a success state.
```bash
dag.trigger_rule = 'one_success'
```
